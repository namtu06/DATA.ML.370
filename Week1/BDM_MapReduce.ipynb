{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e515dc",
   "metadata": {},
   "source": [
    "### Nam Tu - 153076622 - Pyspark and MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8929199e",
   "metadata": {},
   "source": [
    "### Task 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad5b589",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2085c9ea",
   "metadata": {},
   "source": [
    "### Task 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa70ad",
   "metadata": {},
   "source": [
    "1. The code generates numbers from 1 to (amount of numbers per partition) * no.partitions , the entire set is then partitioned into 10 parts. The 10 parts are then passed off onto a MapReduce process which map a truth value to each value (1 if a randomly generated point lies in a circle of radius 1, 0 otherwise). The total number of points is then calculated via aggregation in the reduce stage. From which the value of pi can be estimated  via the formula 4*(no. of points in the circle)/no. of points generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739954c7",
   "metadata": {},
   "source": [
    "2. sc.parrallelize takes the entire dataset and turns it into an RDD with the number of partitions stated (10 in this case), which will enable the ability to do parrallel computations (hence the name). The RDD can then be passed off onto a MapReduce process, making the computation much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f3be8",
   "metadata": {},
   "source": [
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398843aa",
   "metadata": {},
   "source": [
    "4. Increasing the amount of data moves the results towards the true value of pi, decreasing does the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c469670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi  (with 100.000 points) is roughly 3.133760\n",
      "Pi (with 50.000 points) is roughly 3.127840\n",
      "Pi (with 300.000 points) is roughly 3.142160\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from operator import add\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(\"local\", \"CalculatePi\")\n",
    "partitions = 10\n",
    "n1 = 100000 * partitions\n",
    "n2 = 50000 * partitions\n",
    "n3 = 300000 * partitions\n",
    "#function which evaluates if the random x,y point is inside the unit circle\n",
    "def f(_):\n",
    " x = random() * 2 - 1\n",
    " y = random() * 2 - 1\n",
    " return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "#parallelize = make a range of keys and partition keys to x partitions\n",
    "#assing a value to each key with .map(f)\n",
    "#and finally reduce the values by summation\n",
    "rdd1 = sc.parallelize(range(1, n1 + 1), partitions)\n",
    "count1 = rdd1.map(f).reduce(add)\n",
    "print(\"Pi  (with 100.000 points) is roughly %f\" % (4.0 * count1 / n1))\n",
    "\n",
    "rdd2 = sc.parallelize(range(1, n2 + 1), partitions)\n",
    "count2 = rdd2.map(f).reduce(add)\n",
    "print(\"Pi (with 50.000 points) is roughly %f\" % (4.0 * count2 / n2))\n",
    "\n",
    "rdd3 = sc.parallelize(range(1, n3 + 1), partitions)\n",
    "count3 = rdd3.map(f).reduce(add)\n",
    "print(\"Pi (with 300.000 points) is roughly %f\" % (4.0 * count3 / n3))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605301be",
   "metadata": {},
   "source": [
    "### Task 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501f0bb",
   "metadata": {},
   "source": [
    "The text file is fed into the instance of Spark, essentially making the textfile a RDD consisting of strings, Pyspark will do the paritition itself(?). The RDD is fed through 2 mapping functions before reducing. First, each line is formatted: punctuations and tabs removed. Each line is then split into words. Second, for each partition, we count the amount of time each word appears."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87efa05c",
   "metadata": {},
   "source": [
    "Finally, for the reduction stage, merge the count of words from each partition by adding them together. The output is the product after the MapReduce, displaying the words and their count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a276e7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 14512)\n",
      "('project', 87)\n",
      "('gutenberg', 26)\n",
      "('ebook', 12)\n",
      "('of', 6707)\n",
      "('moby', 81)\n",
      "('dick;', 10)\n",
      "('or', 787)\n",
      "('whale', 846)\n",
      "('', 4316)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(\"local\", \"words\")\n",
    "lines = sc.textFile(\"*.txt\")\n",
    "words = lines.map(lambda line:\n",
    "line.replace(\".\",\"\").replace(\",\",\"\").replace(\"?\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\")\n",
    ".replace(\"!\",\"\").replace(\"-\",\"\").lower()).flatMap(lambda line: line.split(\" \"))\n",
    "keyvalue = words.map(lambda word: (word, 1))\n",
    "counts = keyvalue.reduceByKey(lambda a, b: a + b)\n",
    "#retrieve at most 200 key value pairs from the rdd (itâ€™s not wise to try to get everything)\n",
    "for x in counts.take(10):\n",
    " print(x)\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
