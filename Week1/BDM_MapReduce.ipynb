{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e515dc",
   "metadata": {},
   "source": [
    "### Nam Tu - 153076622 - Pyspark and MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8929199e",
   "metadata": {},
   "source": [
    "### Task 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad5b589",
   "metadata": {},
   "source": [
    "A bag (multiset) is a group of objects where a single object can appear multiple times, whereas it can appear only once in a set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98dbf9f",
   "metadata": {},
   "source": [
    "The operation produces the natural join between 2 relations. The first relation is comprised of 2 attributes A, B. The second relation is also comprised of 2 attributes C and D, but the second relation has been applied a condition of only taking tuples whose value in C is larger than that in attribute B of the first relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b30ab",
   "metadata": {},
   "source": [
    "```\n",
    "QUICK IMPLEMENTATION MOCKUP\n",
    "\n",
    "R(A,B)\n",
    "S(C,D)\n",
    "\n",
    "result = []\n",
    "\n",
    "for (a, b) in R:\n",
    "    for (c, d) in S:\n",
    "        if c > b:\n",
    "            result.append((a, b, c, d))\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085c9ea",
   "metadata": {},
   "source": [
    "### Task 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa70ad",
   "metadata": {},
   "source": [
    "1. The code generates numbers from 1 to (amount of numbers per partition) * no.partitions , the entire set is then partitioned into 10 parts. The 10 parts are then passed off onto a MapReduce process which map a truth value to each count (1 if a randomly generated point lies in a circle of radius 1, 0 otherwise). The total number of points that fits the condition is then calculated via aggregation in the reduce stage. From which the value of pi can be estimated  via the formula 4*(no. of points in the circle)/no. of points generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739954c7",
   "metadata": {},
   "source": [
    "2. sc.parrallelize takes the entire dataset and turns it into an RDD with the number of partitions stated (10 in this case), which will enable the ability to do parrallel computations (hence the name). The RDD can then be passed off onto a MapReduce process, making the computation much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f3be8",
   "metadata": {},
   "source": [
    "3. Technically there is no key-value pair since the function only returns a boolean, and the final mapping result is an array of 1's and 0's. If we are assessing theoretically then it follows as such:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c452e02",
   "metadata": {},
   "source": [
    "    The key-value pairs are of the form (number, boolean). Where the number is the count of numbers that have already been passed off into f, and the boolean is 1 if the randomly generation number fits the condition of f, 0 otherwise. The f function can be seen in the implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398843aa",
   "metadata": {},
   "source": [
    "4. Increasing the amount of data moves the results towards the true value of pi, decreasing does the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c469670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi  (with 100.000 points) is roughly 3.133760\n",
      "Pi (with 50.000 points) is roughly 3.127840\n",
      "Pi (with 300.000 points) is roughly 3.142160\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from operator import add\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(\"local\", \"CalculatePi\")\n",
    "partitions = 10\n",
    "n1 = 100000 * partitions\n",
    "n2 = 50000 * partitions\n",
    "n3 = 300000 * partitions\n",
    "#function which evaluates if the random x,y point is inside the unit circle\n",
    "def f(_):\n",
    " x = random() * 2 - 1\n",
    " y = random() * 2 - 1\n",
    " return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "#parallelize = make a range of keys and partition keys to x partitions\n",
    "#assing a value to each key with .map(f)\n",
    "#and finally reduce the values by summation\n",
    "rdd1 = sc.parallelize(range(1, n1 + 1), partitions)\n",
    "count1 = rdd1.map(f).reduce(add)\n",
    "print(\"Pi  (with 100.000 points) is roughly %f\" % (4.0 * count1 / n1))\n",
    "\n",
    "rdd2 = sc.parallelize(range(1, n2 + 1), partitions)\n",
    "count2 = rdd2.map(f).reduce(add)\n",
    "print(\"Pi (with 50.000 points) is roughly %f\" % (4.0 * count2 / n2))\n",
    "\n",
    "rdd3 = sc.parallelize(range(1, n3 + 1), partitions)\n",
    "count3 = rdd3.map(f).reduce(add)\n",
    "print(\"Pi (with 300.000 points) is roughly %f\" % (4.0 * count3 / n3))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605301be",
   "metadata": {},
   "source": [
    "### Task 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501f0bb",
   "metadata": {},
   "source": [
    "The text file is fed into the instance of Spark, essentially making the textfile a RDD consisting of strings, Pyspark will do the paritition itself(?). The RDD is fed through 2 mapping functions before reducing. First, each line is formatted: punctuations and tabs removed. Each line is then split into words. Second, for each partition, we count the amount of time each word appears."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87efa05c",
   "metadata": {},
   "source": [
    "Finally, for the reduction stage, merge the count of words from each partition by adding them together. The output is the product after the MapReduce, displaying the words and their count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a276e7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 14512)\n",
      "('project', 87)\n",
      "('gutenberg', 26)\n",
      "('ebook', 12)\n",
      "('of', 6707)\n",
      "('moby', 81)\n",
      "('dick;', 10)\n",
      "('or', 787)\n",
      "('whale', 846)\n",
      "('', 4316)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(\"local\", \"words\")\n",
    "lines = sc.textFile(\"mobydick.txt\")\n",
    "words = lines.map(lambda line:\n",
    "line.replace(\".\",\"\").replace(\",\",\"\").replace(\"?\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\")\n",
    ".replace(\"!\",\"\").replace(\"-\",\"\").lower()).flatMap(lambda line: line.split(\" \"))\n",
    "keyvalue = words.map(lambda word: (word, 1))\n",
    "counts = keyvalue.reduceByKey(lambda a, b: a + b)\n",
    "#retrieve at most 200 key value pairs from the rdd (itâ€™s not wise to try to get everything)\n",
    "for x in counts.take(10):\n",
    " print(x)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395527d",
   "metadata": {},
   "source": [
    "### Task 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6340e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sc = pyspark.SparkContext(\"local\", \"words2\")\n",
    "\n",
    "lines1 = sc.textFile(\"frankenstein.txt\") \n",
    "lines2 = sc.textFile(\"mobydick.txt\")\n",
    "\n",
    "words1 = (\n",
    "    lines1\n",
    "    .map(lambda line: re.sub(r\"[^a-zA-Z\\s]\", \"\", line)) # remove trash + numbers\n",
    "    .map(lambda line: line.lower())\n",
    "    .flatMap(lambda line: line.split())\n",
    "    .filter(lambda word: word != \"\")    # remove empty words\n",
    ")\n",
    "\n",
    "words2 = (\n",
    "    lines2\n",
    "    .map(lambda line: re.sub(r\"[^a-zA-Z\\s]\", \"\", line)) # remove trash + numbers\n",
    "    .map(lambda line: line.lower())\n",
    "    .flatMap(lambda line: line.split())\n",
    "    .filter(lambda word: word != \"\")    # remove empty words\n",
    ")\n",
    "\n",
    "keyvalue1 = words1.map(lambda word: (word, 1))\n",
    "counts1 = keyvalue1.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "keyvalue2 = words2.map(lambda word: (word, 1))\n",
    "counts2 = keyvalue2.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "\n",
    "\n",
    "top10_1 = counts1.takeOrdered(10, key=lambda x: -x[1])\n",
    "top10_2 = counts2.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "unique_words = set([w for w, _ in top10_1] + [w for w, _ in top10_2])\n",
    "\n",
    "dict1 = dict(counts1.collect())\n",
    "dict2 = dict(counts2.collect())\n",
    "\n",
    "data = []\n",
    "for w in unique_words:\n",
    "    data.append({\n",
    "        \"word\": w,\n",
    "        \"Frankenstein\": dict1.get(w, 0),\n",
    "        \"MobyDick\": dict2.get(w, 0)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data).sort_values(\"word\")\n",
    "\n",
    "\n",
    "df.set_index(\"word\").plot(kind=\"bar\", figsize=(10, 5))\n",
    "\n",
    "plt.title(\"Top word occurrences in Frankenstein vs Moby Dick\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
