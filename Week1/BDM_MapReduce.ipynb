{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e515dc",
   "metadata": {},
   "source": [
    "### Nam Tu - 153076622 - Pyspark and MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8929199e",
   "metadata": {},
   "source": [
    "### Task 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad5b589",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2085c9ea",
   "metadata": {},
   "source": [
    "### Task 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa70ad",
   "metadata": {},
   "source": [
    "1. The code generates numbers from 1 to (amount of numbers per partition) * no.partitions , the entire set is then partitioned into 10 parts. The 10 parts are then passed off onto a MapReduce process which map a truth value to each value (1 if a randomly generated point lies in a circle of radius 1, 0 otherwise). The total number of points is then calculated via aggregation in the reduce stage. From which the value of pi can be estimated  via the formula 4*(no. of points in the circle)/no. of points generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739954c7",
   "metadata": {},
   "source": [
    "2. sc.parrallelize takes the entire dataset and turns it into an RDD with the number of partitions stated (10 in this case), which will enable the ability to do parrallel computations (hence the name). The RDD can then be passed off onto a MapReduce process, making the computation much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f3be8",
   "metadata": {},
   "source": [
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398843aa",
   "metadata": {},
   "source": [
    "4. Increasing the amount of data moves the results towards the true value of pi, decreasing does the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c469670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi  (with 100.000 points) is roughly 3.133760\n",
      "Pi (with 50.000 points) is roughly 3.127840\n",
      "Pi (with 300.000 points) is roughly 3.142160\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from operator import add\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(\"local\", \"CalculatePi\")\n",
    "partitions = 10\n",
    "n1 = 100000 * partitions\n",
    "n2 = 50000 * partitions\n",
    "n3 = 300000 * partitions\n",
    "#function which evaluates if the random x,y point is inside the unit circle\n",
    "def f(_):\n",
    " x = random() * 2 - 1\n",
    " y = random() * 2 - 1\n",
    " return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "#parallelize = make a range of keys and partition keys to x partitions\n",
    "#assing a value to each key with .map(f)\n",
    "#and finally reduce the values by summation\n",
    "rdd1 = sc.parallelize(range(1, n1 + 1), partitions)\n",
    "count1 = rdd1.map(f).reduce(add)\n",
    "print(\"Pi  (with 100.000 points) is roughly %f\" % (4.0 * count1 / n1))\n",
    "\n",
    "rdd2 = sc.parallelize(range(1, n2 + 1), partitions)\n",
    "count2 = rdd2.map(f).reduce(add)\n",
    "print(\"Pi (with 50.000 points) is roughly %f\" % (4.0 * count2 / n2))\n",
    "\n",
    "rdd3 = sc.parallelize(range(1, n3 + 1), partitions)\n",
    "count3 = rdd3.map(f).reduce(add)\n",
    "print(\"Pi (with 300.000 points) is roughly %f\" % (4.0 * count3 / n3))\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
